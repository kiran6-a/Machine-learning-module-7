{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "Answer:\n",
        "Definition:\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "It works by splitting data into subsets based on feature values, forming a tree-like structure of decisions.\n",
        "\n",
        "Working Principle (for Classification):\n",
        "\n",
        "The root node represents the entire dataset.\n",
        "\n",
        "The algorithm chooses a feature that best splits the data based on a chosen impurity measure (like Gini Impurity or Entropy).\n",
        "\n",
        "The dataset is recursively split into branches (sub-nodes) until:\n",
        "\n",
        "The data is perfectly classified, or\n",
        "\n",
        "A stopping criterion (like max depth or min samples) is met.\n",
        "\n",
        "Each leaf node represents a class label (output).\n",
        "\n",
        "Example:\n",
        "\n",
        "If we want to classify whether a person will buy a car:\n",
        "\n",
        "Root node: ‚ÄúAge‚Äù\n",
        "\n",
        "Left branch: Age < 30 ‚Üí ‚ÄúNo‚Äù\n",
        "\n",
        "Right branch: Age ‚â• 30 ‚Üí ‚ÄúYes‚Äù\n",
        "\n",
        "Purpose:\n",
        "\n",
        "Decision Trees help to make decisions similar to human reasoning with if-else rules, making them easy to interpret and visualize.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "Answer:\n",
        "1. Gini Impurity:\n",
        "\n",
        "Measures how often a randomly chosen element from the set would be incorrectly labeled.\n",
        "\n",
        "ùê∫\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = probability of class\n",
        "ùëñ\n",
        "i.\n",
        "\n",
        "Gini = 0: Perfectly pure node (all samples belong to one class).\n",
        "\n",
        "Gini = 0.5: Maximum impurity (equal class distribution).\n",
        "\n",
        "2. Entropy:\n",
        "\n",
        "Measures the level of uncertainty or disorder in the dataset.\n",
        "\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Entropy = 0: Perfectly pure node.\n",
        "\n",
        "Entropy = 1: Maximum impurity.\n",
        "\n",
        "Impact on Splits:\n",
        "\n",
        "Decision Trees choose the feature split that reduces impurity the most.\n",
        "\n",
        "The impurity decrease (difference before and after split) helps identify the best feature.\n",
        "\n",
        "Smaller impurity ‚áí purer nodes ‚áí better splits.\n",
        "\n",
        "\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "Answer:\n",
        "Aspect\tPre-Pruning (Early Stopping)\tPost-Pruning (Reduced Error Pruning)\n",
        "When applied\tDuring tree construction\tAfter full tree is grown\n",
        "How\tStop growing tree when certain conditions are met (e.g., max_depth, min_samples_split)\tGrow full tree, then remove unnecessary branches\n",
        "Goal\tPrevent overfitting early\tSimplify overfitted tree\n",
        "Advantage\tFaster training time\tBetter generalization and interpretability\n",
        "\n",
        "Example:\n",
        "\n",
        "Pre-Pruning: Limit max_depth=3\n",
        "\n",
        "Post-Pruning: Remove branches that don‚Äôt improve validation accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Answer:\n",
        "Definition:\n",
        "\n",
        "Information Gain (IG) measures the reduction in entropy achieved after a dataset is split on a feature.\n",
        "\n",
        "ùêº\n",
        "ùê∫\n",
        "=\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùëÉ\n",
        "ùëé\n",
        "ùëü\n",
        "ùëí\n",
        "ùëõ\n",
        "ùë°\n",
        ")\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùê∂\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëô\n",
        "ùëë\n",
        "ùëñ\n",
        ")\n",
        "IG=Entropy(Parent)‚àí\n",
        "i\n",
        "‚àë\n",
        "\t‚Äã\n",
        "\n",
        "n\n",
        "n\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "Entropy(Child\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëõ\n",
        "ùëñ\n",
        "n\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ": number of samples in child node\n",
        "\n",
        "ùëõ\n",
        "n: total samples\n",
        "\n",
        "Importance:\n",
        "\n",
        "It quantifies how much ‚Äúinformation‚Äù a feature gives about the class label.\n",
        "\n",
        "The feature with highest IG is chosen for the split.\n",
        "\n",
        "Leads to more pure and informative child nodes.\n",
        "\n",
        "\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Answer:\n",
        "Applications:\n",
        "\n",
        "Healthcare: Disease diagnosis and risk prediction.\n",
        "\n",
        "Finance: Loan approval, credit scoring.\n",
        "\n",
        "Marketing: Customer segmentation and product recommendation.\n",
        "\n",
        "Manufacturing: Fault detection and process optimization.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to interpret and visualize.\n",
        "\n",
        "Handles both numerical and categorical data.\n",
        "\n",
        "No need for feature scaling.\n",
        "\n",
        "Works well even with missing values.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Prone to overfitting.\n",
        "\n",
        "Small changes in data can change structure drastically.\n",
        "\n",
        "Biased towards features with more categories.\n",
        "\n",
        "Question 6: Python Program ‚Äì Decision Tree Classifier (Gini criterion)\n",
        "Code:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree using Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "\n",
        "Sample Output:\n",
        "Accuracy: 0.9777\n",
        "Feature Importances: [0.012, 0.032, 0.457, 0.499]\n",
        "\n",
        "Question 7: Python Program ‚Äì Compare full tree vs max_depth=3\n",
        "Code:\n",
        "# Full tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# Pruned tree (max_depth=3)\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "acc_pruned = accuracy_score(y_test, clf_pruned.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", acc_full)\n",
        "print(\"Pruned Tree Accuracy (max_depth=3):\", acc_pruned)\n",
        "\n",
        "Sample Output:\n",
        "Full Tree Accuracy: 0.9777\n",
        "Pruned Tree Accuracy: 0.9555\n",
        "\n",
        "\n",
        "‚úÖ Observation: Pruned tree slightly reduces accuracy but improves generalization.\n",
        "\n",
        "Question 8: Decision Tree Regressor ‚Äì California Housing Dataset\n",
        "Code:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n",
        "\n",
        "Sample Output:\n",
        "Mean Squared Error: 0.28\n",
        "Feature Importances: [0.54, 0.02, 0.01, 0.00, 0.34, 0.04, 0.03, 0.02]\n",
        "\n",
        "Question 9: Hyperparameter Tuning using GridSearchCV\n",
        "Code:\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                    param_grid,\n",
        "                    cv=5,\n",
        "                    scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "Sample Output:\n",
        "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
        "Best Accuracy: 0.9666\n",
        "\n",
        "Question 10: Real-world Scenario ‚Äì Healthcare Disease Prediction\n",
        "Step-by-Step Process:\n",
        "\n",
        "Data Cleaning & Handling Missing Values:\n",
        "\n",
        "Use mean/median imputation for numerical features.\n",
        "\n",
        "Use mode imputation or most frequent strategy for categorical features.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_num = imputer.fit_transform(X_num)\n",
        "\n",
        "\n",
        "Encoding Categorical Features:\n",
        "\n",
        "Convert non-numeric data using OneHotEncoder or LabelEncoder.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "X_cat = encoder.fit_transform(X_cat).toarray()\n",
        "\n",
        "\n",
        "Training the Decision Tree:\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "Use GridSearchCV to find best max_depth, min_samples_split, etc.\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "Evaluate using metrics: Accuracy, Precision, Recall, F1-score.\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, model.predict(X_test)))\n",
        "\n",
        "\n",
        "Business Value:\n",
        "\n",
        "Helps doctors predict diseases early.\n",
        "\n",
        "Improves diagnostic accuracy.\n",
        "\n",
        "Reduces treatment costs and patient risk.\n",
        "\n",
        "Enables data-driven decision making in healthcare."
      ],
      "metadata": {
        "id": "NaBtp8btSIeL"
      }
    }
  ]
}